# 搭建本项目实验环境

- [搭建本项目实验环境](#搭建本项目实验环境)
  - [一、实验目的](#一实验目的)
  - [二、云服务器购买与配置](#二云服务器购买与配置)
  - [三、单机版环境搭建](#三单机版环境搭建)
    - [第一步：更新软件安装包](#第一步更新软件安装包)
    - [第二步：配置 SSH 免密登录](#第二步配置-ssh-免密登录)
    - [第三步：安装 Java 并配置环境变量](#第三步安装-java-并配置环境变量)
    - [第四步：安装与配置 Hadoop](#第四步安装与配置-hadoop)
    - [第五步：安装与配置 Spark](#第五步安装与配置-spark)
  - [四、分布式环境搭建](#四分布式环境搭建)

## 一、实验目的

核心目标：完成 Hadoop & Spark 单机版以及分布式环境搭建。

最终需搭建相关详细环境如下：

1. 操作系统：Ubuntu 22.04 LTS
2. 辅助工具：Xshell 7, Xftp 7 以及 Xmanager 7
3. 语言环境：OpenJDK 1.8.0
4. 相关软件：Hadoop 3.3.6 以及 Spark 3.5.1

## 二、云服务器购买与配置

本项目使用**阿里云**提供的云服务器，操作系统选**Ubuntu 22.04 LTS**，配置云服务器较为简单，不在此详细说明，强烈建议跟着[实验零：Linux 初识](https://github.com/Wanghui-Huang/CQU_bigdata/blob/master/Experiment/Ex0_HelloLinux/ex0.md)走一遍！大部分是一致的，只有云厂商和操作系统有些许差别。

> **购机提示**：
>
> 1. 因为后面实验任务会很吃云服务器配置，所以建议选购时**在条件允许的范围内尽可能选择较高配置的**，如按量计费**4核(vCPU) 8GiB**左右，记得随用随开**不用即关**，以免多扣费！
> 2. 地域选择建议选择**离重庆近的**，如**西南-成都**等可用区。
> 3. 设置登录用户名与密码时，强烈建议**用户名不用`root`而用`ecs-user`**，保障使用安全同时实验中某些操作是不允许`root`用户进行的。

## 三、单机版环境搭建

此刻，你应当购买且配好了云服务器与Xshell等工具，且已经用Xshell 7连上了自己的云服务器，面前应该就是一台空白的机器了，下面将开始**使用命令行**配置你的机器！

![你目前的状态](images/your%20current%20status.jpg)

> **配置前的注意事项**：如果你当前为`root`用户，你必须切换为非`root`用户才能正常使用 hadoop 集群，否则不能使用。当然，很大可能你在购买云服务器时已经被阿里云官方推荐或听从购机提示第 $3$ 点使用`ecs-user`用户，就不必再做这一步了！

切换为非`root`用户：

```bash
sudo adduser ecs-user
sudo usermod -aG sudo ecs-user
su - ecs-user
```

### 第一步：更新软件安装包

```bash
sudo apt-get update && sudo apt-get upgrade -y
```

这一步是在进行任何配置之前都要做的，特别是对于一台全新机器时，这一步将进行较长时间！

***

### 第二步：配置 SSH 免密登录

> **为什么要配置 SSH 免密登录？**
> 后面搭建分布式集群时，主节点需要分发与调度任务，涉及跨节点访问与通信，免密登录使其自动化。

1. 生成 SSH 密钥

    ```bash
    ssh-keygen -t rsa -P "" -f ~/.ssh/id_rsa
    ```

2. 将公钥添加到 `authorized_keys` 并提供权限

    ```bash
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
    chmod 600 ~/.ssh/authorized_keys
    ```

3. 测试本地主机能否无密码登录成功

    ```bash
    ssh localhost
    ```

4. 命令执行后显示的是你一开始的登录界面，说明配置成功
    ![ssh localhost](images/your%20current%20status.jpg)

5. 为远程主机配置无密码访问（以我的 `CQULeaf` 为例，你需要替换成对应的主机名 `hostname`）

    ```bash
    ssh-copy-id ecs-user@CQULeaf
    ```

6. 测试远程主机无密码登录

    ```bash
    ssh CQULeaf
    ```

    命令执行后显示的是你一开始的登录界面，说明配置成功
    ![ssh CQULeaf](images/your%20current%20status.jpg)

### 第三步：安装 Java 并配置环境变量

1. 安装Java

    ```bash
    sudo apt-get install openjdk-8-jdk -y
    ```

2. 检验是否安装成功

    ```bash
    java -version
    ```

    若操作成功，会显示如下版本信息
    ![Java 版本信息](images/java%20version.jpg)

    注意，你此时其实还并未配置 `JAVA_HOME` 环境变量，虽然你的确能够使用 `java -version` 查看 Java 版本。`JAVA_HOME` 环境变量通常用于一些需要明确知道 Java 安装目录的应用程序（如我们将要使用的 Hadoop、Spark），所以我们**仍然**要进行环境变量的配置。

3. 配置 `JAVA_HOME` 环境变量

    ```bash
    nano ~/.bashrc
    ```

    在最下方随便找个空白位置键入如下内容

    ```bash
    export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
    export PATH=$PATH:$JAVA_HOME/bin
    ```

    键入完毕，按 `Ctrl O` 保存，回车一下，再按 `Ctrl X` 退出。

    退出后运行下述命令使配置生效

    ```bash
    source ~/.bashrc
    ```

    验证环境变量是否正常配置

    ```bash
    echo $JAVA_HOME
    ```

    正常情况下会显示如下内容

    ![java path](images/java%20path.jpg)

    >如果你严格按指南进行，则大概率可以直接复制粘贴环境变量内容，但如果你不确定自己的 Java 位置，可以输入如下指令查看

    ```bash
    sudo update-alternatives --config java
    ```

***

### 第四步：安装与配置 Hadoop

> 注意下载Hadoop安装包我们将选择**华为源**，下载速度快且版本全。

1. 下载并安装Hadoop

    1. 下载安装包

        ```bash
        wget --no-check-certificate https://repo.huaweicloud.com/apache/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
        ```

    2. 解压安装包

        ```bash
        tar -xzf hadoop-3.3.6.tar.gz
        ```

    3. 移动解压后的文件夹位置（可选但强烈建议，以便系统管理）

        > 如果你选择不移动文件夹位置，则注意在下面配置系统环境变量时填入正确的自己的路径。而如果选择按我这样移动，则可以直接复制粘贴对应的环境变量内容！

        ```bash
        sudo mv hadoop-3.3.6 /usr/local/hadoop
        ```

        > 解压后你可以选择删除压缩包

        ```bash
        rm hadoop-3.3.6.tar.gz
        ```

2. 配置 `HADOOP_HOME` 环境变量

    1. 打开对应文件

        ```bash
        nano ~/.bashrc
        ```

    2. 在最下方加上如下内容

        ```bash
        # Hadoop environment variables
        export HADOOP_HOME=/usr/local/hadoop
        export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
        export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
        ```

    3. 键入完毕，按 `Ctrl O` 保存，回车一下，再按 `Ctrl X` 退出。

    4. 退出后运行下述命令使配置生效

        ```bash
        source ~/.bashrc
        ```

    5. 验证环境变量是否正常配置

        ```bash
        echo $HADOOP_HOME
        ```

        成功配置将显示以下内容
        ![hadoop path](images/hadoop%20path.jpg)

3. 验证 Hadoop 是否安装成功

    ```bash
    hadoop version
    ```

    成功配置将显示以下内容

    ![hadoop version](images/hadoop%20version.jpg)

4. 至此，hadoop 配置完成！

***

### 第五步：安装与配置 Spark

1. 下载并安装 Spark

   1. 下载安装包

        ```bash
        wget --no-check-certificate https://repo.huaweicloud.com/apache/spark/spark-3.5.1/spark-3.5.1-bin-without-hadoop.tgz
        ```

   2. 解压安装包

        ```bash
        tar -xvzf spark-3.5.1-bin-without-hadoop.tgz
        ```

   3. 移动解压后的文件夹位置，并删除压缩包

        ```bash
        sudo mv spark-3.5.1-bin-without-hadoop /usr/local/spark
        rm spark-3.5.1-bin-without-hadoop.tgz
        ```

2. 配置 `SPARK_HOME` 环境变量

    1. 打开对应文件

        ```bash
        nano ~/.bashrc
        ```

    2. 在最下方加上如下内容

        ```bash
        # Spark environment variables
        export SPARK_HOME=/usr/local/spark
        export PYSPARK_PYTHON=/usr/bin/python3
        export PATH=$PATH:$SPARK_HOME/bin
        export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH
        ```

    3. 键入完毕，按 `Ctrl O` 保存，回车一下，再按 `Ctrl X` 退出。

    4. 退出后运行下述命令使配置生效

        ```bash
        source ~/.bashrc
        ```

    5. 验证环境变量是否正常配置

        ```bash
        echo $SPARK_HOME
        ```

        成功配置将显示以下内容
        ![spark path](images/spark%20path.png)

3. 配置 Spark 环境

    1. 切换至 `/usr/local/spark` 并复制模版文件。

        ```bash
        cd /usr/local/spark
        cp ./conf/spark-env.sh.template ./conf/spark-env.sh
        ```

    2. 编辑 `spark-env.sh` 文件

        ```bash
        vim ./conf/spark-env.sh
        ```

    3. 在任意位置添加下面配置信息，使得 Spark 可以从 Hadoop 读取数据。

        ```bash
        export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)
        ```

4. 测试 Spark 是否运行成功

    ```bash
    bin/run-example SparkPi
    ```

    看见终端输出以下信息即表明 Spark 运行成功，说明上述配置操作无误！
    ![spark result](images/spark%20test%20result.png)

5. 测试 PySpark 运行情况

    ```bash
    bin/pyspark
    ```

    出现如下图交互情况说明 PySpark 也运行成功！
    ![pyspark result](images/pyspark%20result.png)

6. 至此 Spark 配置完成！

***

## 四、分布式环境搭建

> 分布式的搭建需要你首先成功配置好单机版环境，同时还要明白我们搭建 Hadoop + Spark 分布式的目的是：使用 Hadoop 的 HDFS 来存储数据，使用 Spark 高效的内存计算（代替 MapReduce）来分析数据，使用 Hadoop 的 yarn 来管理资源，使用多台云服务器来分配计算任务。

1. 配置 Hadoop
    如果你不熟悉 vim，我推荐使用 Xftp 7 直接在记事本上修改，简单粗暴！（当然你也可以命令行中使用vim来修改文件内容）

    > 聪明的你肯定也会意识到，刚刚配置环境变量也可以直接在记事本上修改！！

    你需要导航到这个界面（通过在Xftp 7中点点点）
    ![the file path](images/the%20file%20path.jpg)
    然后关注四个文件，`hadoop-env.sh`、`yarn-env.sh`以及`core-site.xml`、`hdfs-site.xml`

    - 在`hadoop-env.sh`与`yarn-env.sh`中
    在文件最后面加上一行（两者同理）

        ```bash
        export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
        ```

        ![修改环境配置文件](images/edit%20env%20file.jpg)

    - 在`core-site.xml`、`hdfs-site.xml`中

        在`core-site.xml`文件后面加上这些：

        ```xml
        <configuration>
            <property>
                <name>hadoop.tmp.dir</name>
                <value>file:/opt/hadoop/tmp</value>
                <description>location to store temporary files</description>
            </property>
            <property>
                <name>fs.defaultFS</name>
                <value>hdfs://localhost:9000</value>
            </property>
        </configuration>
        ```

        实际情况如下

        ![core-site](images/edit%20core-site.jpg)

        在`hdfs-site.xml`文件后面加上这些：

        ```xml
        <configuration>
            <property>
                <name>dfs.replication</name>
                <value>1</value>
            </property>
            <property>
                <name>dfs.namenode.name.dir</name>
                <value>file:/opt/hadoop/tmp/dfs/name</value>
            </property>
            <property>
                <name>dfs.datanode.data.dir</name>
                <value>file:/opt/hadoop/tmp/dfs/data</value>
            </property>
        </configuration>
        ```

        实际情况如下
        ![hdfs-site](images/edit%20hdfs-site.jpg)

2. 解决一些权限问题
    如果你当前用户非 `root` 用户，比如我目前是 `ecs-user`，需要解决一些权限问题。

    > 但请注意，在开通云服务器并创建用户时，阿里云就建议使用 `ecs-user` 而不要直接使用 `root`，太高的权限并不一定好。
  
   1. 解决 hadoop 日志目录权限问题

       ```bash
       sudo mkdir -p /usr/local/hadoop/logs
       sudo chown -R ecs-user:ecs-user /usr/local/hadoop/logs
       ```

       确保当前用户（ecs-user）拥有对日志目录的写权限。

   2. 解决设置进程优先级问题

        非 `root` 用户没有足够的权限来通过 `nice` 命令更改进程的优先级，会导致一会儿运行 hadoop 报错。

        还是修改 `hadoop-env.sh` 文件（上面已经改过一次了）

        ![edit niceness](images/edit%20niceness.jpg)

        使用Xftp 7用记事本打开 `hadoop-env.sh` 文件，找到`# export HADOOP_NICENESS=0`这一行，**去掉注释**即可！（如上图，只有一行没有注释，很容易看出来）

3. 云服务器打开相应端口

    请在云厂商云服务器管理控制台的安全组中开放**以下所有**端口，否则无法正常运行 hadoop 与 spark

    ![safety group](images/safety%20group.jpg)

    > 开放的端口不仅包含 hadoop 必需的，还有 spark 必需的，所以显得有点多，不过下面 spark 配置时就不用再来开放端口了。
    > 如果运行不成功，有很大可能是端口有些没有放通哦~

4. 运行 hadoop

   1. 启动 HDFS

       ```bash
       start-dfs.sh
       ```

       正确运行结果如下：
       ![start-dfs](images/start-dfs.jpg)

   2. 启动 YARN

       ```bash
       start-yarn.sh
       ```

       正确运行结果如下：
       ![start-yarn](images/start-yarn.jpg)

   3. 检查进程

       ```bash
       jps
       ```

       正确运行结果如下：
       ![jps](images/jps.jpg)

       > 正常会有 $6$ 个进程，缺一不可！！说明有问题，你没有配置成功！！

   4. 检查 Hadoop Web UI

       先检查 HDFS NameNode Web UI，查看 HDFS 状态。

       - 浏览器输入`http://<your-server-ip>:9870/`，将`<your-server-ip>`改为你的云服务器的公网ip地址即可。

       - 显示如下界面说明成功：
       ![HDFS NameNode Web UI](images/HDFS%20NameNode%20Web%20UI.png)

       再检查 YARN ResourceManager Web UI，查看 YARN 状态。

       - 浏览器输入`http://<your-server-ip>:8088/`，将`<your-server-ip>`改为你的云服务器的公网ip地址即可。

       - 显示如下界面说明成功：
       ![YARN ResourceManager Web UI](images/YARN%20ResourceManager%20Web%20UI.jpg)

5. 关闭 hadoop

    > 因为云服务器特性，如果你不手动关闭 hadoop，即使关了 Shell 软件仍然在运行，所以当你不用的时候可以手动关闭 hadoop

   1. 关闭 HDFS

       ```bash
       stop-dfs.sh
       ```

       正确运行结果如下：
       ![stop-dfs](images/stop-dfs.jpg)

   2. 启动 YARN

       ```bash
       stop-yarn.sh
       ```

       正确运行结果如下：
       ![stop-yarn](images/stop-yarn.jpg)

   3. 检查进程

       ```bash
       jps
       ```

       正确运行结果如下：
       ![stop-jps](images/stop-jps.jpg)

       > $5$ 个进程没有了！！说明关闭成功！！

6. 至此 Hadoop 配置完成！！！！
